import os
import argparse
import pandas as pd
import glob
from datetime import datetime
import json
from openai import OpenAI

def load_prediction_files(directory, pattern="*_predictions_*.csv"):
    """
    Find and load all prediction files generated by the log analysis pipeline
    """
    # Find all prediction files in the directory
    prediction_files = glob.glob(os.path.join(directory, pattern))
    
    if not prediction_files:
        print(f"No prediction files found in {directory} matching pattern {pattern}")
        return []
    
    print(f"Found {len(prediction_files)} prediction files")
    return prediction_files

def analyze_prediction_file(file_path):
    """
    Generate a summary analysis for a single prediction file
    """
    try:
        print(f"Analyzing: {file_path}")
        df = pd.read_csv(file_path)
        
        # Basic statistics
        total_records = len(df)
        prediction_counts = df['predicted_category'].value_counts().to_dict()
        prediction_percentages = {k: round(v/total_records*100, 2) for k, v in prediction_counts.items()}
        
        # Extract and analyze additional fields
        log_entries = []
        for _, row in df.iterrows():
            entry = {
                'timestamp': None,
                'source_ip': None,
                'type': row['predicted_category'],
                'sensitivity': 'HIGH' if any(prob > 0.9 for prob in [row[col] for col in df.columns if col.startswith('prob_')]) else 'MEDIUM' if any(prob > 0.7 for prob in [row[col] for col in df.columns if col.startswith('prob_')]) else 'LOW',
                'status': 'ALERT' if row['predicted_category'] in ['attack', 'anomaly', 'threat'] else 'INFO'
            }
            
            # Try to find timestamp
            timestamp_cols = [col for col in df.columns if any(time_indicator in col.lower() 
                                                             for time_indicator in ['time', 'date', 'timestamp'])]
            if timestamp_cols:
                try:
                    entry['timestamp'] = pd.to_datetime(row[timestamp_cols[0]]).isoformat()
                except:
                    pass
            
            # Try to find source IP
            ip_cols = [col for col in df.columns if any(ip_indicator in col.lower() 
                                                       for ip_indicator in ['ip', 'source', 'src', 'address'])]
            if ip_cols:
                entry['source_ip'] = str(row[ip_cols[0]])
            
            log_entries.append(entry)
        
        # Generate time-based trends if timestamps are available
        time_series = None
        if timestamp_cols:
            try:
                df[timestamp_cols[0]] = pd.to_datetime(df[timestamp_cols[0]])
                df = df.sort_values(by=timestamp_cols[0])
                # Group by hour and count incidents
                time_series = df.groupby(pd.Grouper(key=timestamp_cols[0], freq='H'))['predicted_category'].value_counts().unstack().fillna(0).to_dict()
            except:
                print(f"Could not process timestamps in {timestamp_cols[0]}")
        
        # Check for high confidence predictions
        high_confidence_threshold = 0.9
        prob_cols = [col for col in df.columns if col.startswith('prob_')]
        high_confidence_counts = {}
        if prob_cols:
            for col in prob_cols:
                category = col.replace('prob_', '')
                high_confidence_counts[category] = sum(df[col] > high_confidence_threshold)
        
        # Generate summary
        summary = {
            'file_name': os.path.basename(file_path),
            'total_records': total_records,
            'prediction_counts': prediction_counts,
            'prediction_percentages': prediction_percentages,
            'high_confidence_predictions': high_confidence_counts,
            'time_based_analysis': time_series,
            'log_entries': log_entries,
            'summary_stats': {
                'high_sensitivity_count': sum(1 for entry in log_entries if entry['sensitivity'] == 'HIGH'),
                'medium_sensitivity_count': sum(1 for entry in log_entries if entry['sensitivity'] == 'MEDIUM'),
                'low_sensitivity_count': sum(1 for entry in log_entries if entry['sensitivity'] == 'LOW'),
                'alert_count': sum(1 for entry in log_entries if entry['status'] == 'ALERT'),
                'info_count': sum(1 for entry in log_entries if entry['status'] == 'INFO')
            }
        }
        
        return summary
    
    except Exception as e:
        print(f"Error analyzing file {file_path}: {str(e)}")
        return None

def generate_ai_insights(summaries, api_key):
    """
    Use OpenAI to generate insights based on the analysis summaries
    """
    try:
        # Only proceed if API key is provided
        if not api_key:
            print("GitHub token not provided. Skipping AI insights.")
            return None
            
        client = OpenAI(api_key=api_key)
        
        # Prepare the data for the AI
        prompt = f"""
        I have the following log analysis results:
        
        {json.dumps(summaries, indent=2)}
        
        Please provide:
        1. A concise summary of the key findings
        2. Any notable patterns or anomalies
        3. Recommendations based on the data
        4. Security risk assessment based on sensitivity levels
        
        Focus on security implications and potential threats.
        """
        
        response = client.chat.completions.create(
            model="gpt-4",  # or another appropriate model
            messages=[
                {"role": "system", "content": "You are a security log analysis expert. Provide concise, actionable insights."},
                {"role": "user", "content": prompt}
            ],
            temperature=0.7,
            max_tokens=1500
        )
        
        return response.choices[0].message.content
        
    except Exception as e:
        print(f"Error generating AI insights: {str(e)}")
        return f"Could not generate AI insights: {str(e)}"

def save_summary(summaries, ai_insights, output_file=None):
    """
    Save the summary analysis to a file
    """
    if not output_file:
        timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
        output_file = f"log_analysis_summary_{timestamp}.json"
    
    # Create complete report
    report = {
        'generated_at': datetime.now().isoformat(),
        'file_summaries': summaries,
        'ai_insights': ai_insights,
        'meta': {
            'total_files_analyzed': len(summaries),
            'total_records_analyzed': sum(s['total_records'] for s in summaries),
            'high_sensitivity_total': sum(s['summary_stats']['high_sensitivity_count'] for s in summaries),
            'alert_status_total': sum(s['summary_stats']['alert_count'] for s in summaries)
        }
    }
    
    # Save to file
    with open(output_file, 'w') as f:
        json.dump(report, f, indent=2)
    
    print(f"Summary saved to: {output_file}")
    return output_file

def main():
    # Parse command line arguments
    parser = argparse.ArgumentParser(description='Generate summaries from log analysis results.')
    parser.add_argument('--input', type=str, required=True, 
                        help='Directory containing prediction CSV files')
    parser.add_argument('--output', type=str, default=None, 
                        help='Output file for the summary')
    parser.add_argument('--github-token', type=str, default=os.environ.get('GITHUB_TOKEN'),
                        help='GitHub token for AI insights (can also be set via GITHUB_TOKEN env variable)')
    
    args = parser.parse_args()
    
    print(f"Looking for prediction files in: {args.input}")
    
    # Load prediction files
    prediction_files = load_prediction_files(args.input)
    if not prediction_files:
        print("No prediction files found")
        return
    
    print(f"Found files: {prediction_files}")
    
    # Analyze each file
    summaries = []
    for file_path in prediction_files:
        print(f"\nAnalyzing file: {file_path}")
        summary = analyze_prediction_file(file_path)
        if summary:
            summaries.append(summary)
            print("Analysis completed successfully")
        else:
            print("Analysis failed for this file")
    
    if not summaries:
        print("No valid summaries generated")
        return
    
    print(f"\nGenerated {len(summaries)} summaries")
    
    # Generate AI insights if GitHub token is available
    ai_insights = generate_ai_insights(summaries, args.github_token)
    
    # Save the complete summary
    output_file = save_summary(summaries, ai_insights, args.output)
    
    # Print insights to console
    if ai_insights:
        print("\n=== AI-Generated Insights ===")
        print(ai_insights)
    else:
        print("\nSummary statistics:")
        for summary in summaries:
            print(f"\nFile: {summary['file_name']}")
            print(f"Total records: {summary['total_records']}")
            print("Predictions:")
            for category, count in summary['prediction_counts'].items():
                percentage = summary['prediction_percentages'][category]
                print(f"  {category}: {count} ({percentage}%)")
            print("\nSensitivity Levels:")
            print(f"  HIGH: {summary['summary_stats']['high_sensitivity_count']}")
            print(f"  MEDIUM: {summary['summary_stats']['medium_sensitivity_count']}")
            print(f"  LOW: {summary['summary_stats']['low_sensitivity_count']}")
            print(f"  Alerts: {summary['summary_stats']['alert_count']}")
            print(f"  Info: {summary['summary_stats']['info_count']}")

if __name__ == "__main__":
    main() 